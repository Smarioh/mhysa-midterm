{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b96434-5750-4a93-bb1f-55e857a485bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mariohysa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 11 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "Loading full training data...\n",
      "Loading test data...\n",
      "Reducing the training dataset size for memory constraints...\n",
      "Extracting features from training data...\n",
      "Applying sentiment analysis...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd73bc7da944425281f3626956824276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=154322), Label(value='0 / 154322')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing HelpfulnessRatio...\n",
      "Computing product and user average scores...\n",
      "Handling missing values...\n",
      "Feature extraction on training data complete.\n",
      "Preparing training data...\n",
      "Computing overall product and user average scores from training data...\n",
      "Computing TF-IDF features...\n",
      "TF-IDF feature computation complete.\n",
      "Combining numeric and text features...\n",
      "Training logistic regression model with best hyperparameters...\n",
      "Model training complete.\n",
      "Evaluating model with cross-validation...\n",
      "Cross-validation accuracy: 0.6642\n",
      "Merging test data with full train data to get 'Text' column...\n",
      "Warning: 8 entries in test data do not have corresponding 'Text' in train data.\n",
      "Extracting features from test data...\n",
      "Applying sentiment analysis...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6d80418f774e30b1ac0856b97ed919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=19291), Label(value='0 / 19291')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing HelpfulnessRatio...\n",
      "Using precomputed average scores for 'ProductAvgScore' and 'UserAvgScore'.\n",
      "Handling missing values...\n",
      "Feature extraction for test data complete.\n",
      "Preparing test features...\n",
      "Combining test numeric and text features...\n",
      "Predicting scores on test data...\n",
      "Saving predictions to CSV...\n",
      "Score prediction complete; file saved as 'option2.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import nltk\n",
    "import warnings\n",
    "from pandarallel import pandarallel \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# Load full training data\n",
    "print(\"Loading full training data...\")\n",
    "train_data_full = pd.read_csv('train.csv')\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Reduce the dataset size for memory constraints\n",
    "print(\"Reducing the training dataset size for memory constraints...\")\n",
    "train_data = train_data_full.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Ensure 'Text' column exists and is string type in train_data\n",
    "if 'Text' not in train_data.columns:\n",
    "    print(\"Warning: 'Text' column not found in train data.\")\n",
    "    train_data['Text'] = ''\n",
    "train_data['Text'] = train_data['Text'].fillna('').astype(str)\n",
    "\n",
    "# Apply feature extraction on training data\n",
    "def extract_features(df, is_test=False, product_avg_score=None, user_avg_score=None):\n",
    "    # Ensure 'Text' column exists and is string type\n",
    "    if 'Text' not in df.columns:\n",
    "        print(\"Warning: 'Text' column not found. Creating empty 'Text' column.\")\n",
    "        df['Text'] = ''\n",
    "    df['Text'] = df['Text'].fillna('').astype(str)\n",
    "\n",
    "    # Apply sentiment analysis and extract all scores\n",
    "    print(\"Applying sentiment analysis...\")\n",
    "    def sentiment_scores(text):\n",
    "        scores = analyzer.polarity_scores(text)\n",
    "        return pd.Series({\n",
    "            'Sentiment_Neg': scores['neg'],\n",
    "            'Sentiment_Neu': scores['neu'],\n",
    "            'Sentiment_Pos': scores['pos'],\n",
    "            'Sentiment_Compound': scores['compound']\n",
    "        })\n",
    "    sentiment_df = df['Text'].parallel_apply(sentiment_scores)\n",
    "    df = pd.concat([df.reset_index(drop=True), sentiment_df.reset_index(drop=True)], axis=1)\n",
    "    # sentiment_df.reset_index(drop=True, inplace=True)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # df = pd.concat([df, sentiment_df], axis=1)\n",
    "\n",
    "    df['TextLength'] = df['Text'].str.len()\n",
    "    df['ExclamationCount'] = df['Text'].str.count(\"!\")\n",
    "    df['QuestionCount'] = df['Text'].str.count(r\"\\?\")\n",
    "    df['CapitalRatio'] = df['Text'].str.count(r'[A-Z]') / (df['TextLength'] + 1)\n",
    "    # Count uppercase words\n",
    "    df['UppercaseWords'] = df['Text'].str.count(r'\\b[A-Z]{2,}\\b')\n",
    "\n",
    "    # Map sentiment to a 1-5 range\n",
    "    df['SentimentMapped'] = ((df['Sentiment_Compound'] + 1) * 2) + 1  # Maps range -1 to 1 onto 1 to 5\n",
    "\n",
    "    # Convert Sentiment into integer levels\n",
    "    df['SentimentLevel'] = df['Sentiment_Compound'].apply(\n",
    "        lambda x: 1 if x <= -0.6 else 2 if x <= -0.2 else 3 if x <= 0.2 else 4 if x <= 0.6 else 5\n",
    "    )\n",
    "\n",
    "    # Compute HelpfulnessRatio if columns are present\n",
    "    if 'HelpfulnessNumerator' in df.columns and 'HelpfulnessDenominator' in df.columns:\n",
    "        print(\"Computing HelpfulnessRatio...\")\n",
    "        df['HelpfulnessRatio'] = df['HelpfulnessNumerator'] / (df['HelpfulnessDenominator'] + 1)\n",
    "    else:\n",
    "        print(\"Warning: 'HelpfulnessNumerator' or 'HelpfulnessDenominator' column not found. Default 'HelpfulnessRatio' will be used.\")\n",
    "        df['HelpfulnessRatio'] = 0.0\n",
    "\n",
    "    # Interaction features\n",
    "    df['Sentiment_Helpfulness'] = df['Sentiment_Compound'] * df['HelpfulnessRatio']\n",
    "    df['Sentiment_TextLength'] = df['Sentiment_Compound'] * df['TextLength']\n",
    "\n",
    "    # Product/User average score as a feature\n",
    "    if not is_test and 'Score' in df.columns:\n",
    "        # Ensure 'Score' is numeric\n",
    "        df['Score'] = pd.to_numeric(df['Score'], errors='coerce')\n",
    "        # Drop rows with NaN 'Score' before computing averages\n",
    "        df_non_null = df.dropna(subset=['Score'])\n",
    "        print(\"Computing product and user average scores...\")\n",
    "        if 'ProductId' in df.columns:\n",
    "            product_avg_score = df_non_null.groupby('ProductId')['Score'].mean().reset_index().rename(columns={'Score': 'ProductAvgScore'})\n",
    "            # Merge these scores back into the main DataFrame\n",
    "            df = df.merge(product_avg_score, on='ProductId', how='left')\n",
    "        else:\n",
    "            print(\"Warning: 'ProductId' column not found. Skipping 'ProductAvgScore' computation.\")\n",
    "            df['ProductAvgScore'] = df['Score'].mean()\n",
    "\n",
    "        if 'UserId' in df.columns:\n",
    "            user_avg_score = df_non_null.groupby('UserId')['Score'].mean().reset_index().rename(columns={'Score': 'UserAvgScore'})\n",
    "            # Merge these scores back into the main DataFrame\n",
    "            df = df.merge(user_avg_score, on='UserId', how='left')\n",
    "        else:\n",
    "            print(\"Warning: 'UserId' column not found. Skipping 'UserAvgScore' computation.\")\n",
    "            df['UserAvgScore'] = df['Score'].mean()\n",
    "\n",
    "    else:\n",
    "        print(\"Using precomputed average scores for 'ProductAvgScore' and 'UserAvgScore'.\")\n",
    "        if 'ProductId' in df.columns and product_avg_score is not None:\n",
    "            df = df.merge(product_avg_score, on='ProductId', how='left')\n",
    "        else:\n",
    "            print(\"Warning: 'ProductId' column not found or 'product_avg_score' not provided. Using default.\")\n",
    "            df['ProductAvgScore'] = train_data['Score'].mean()\n",
    "\n",
    "        if 'UserId' in df.columns and user_avg_score is not None:\n",
    "            df = df.merge(user_avg_score, on='UserId', how='left')\n",
    "        else:\n",
    "            print(\"Warning: 'UserId' column not found or 'user_avg_score' not provided. Using default.\")\n",
    "            df['UserAvgScore'] = train_data['Score'].mean()\n",
    "\n",
    "    # Handle missing values in the features\n",
    "    print(\"Handling missing values...\")\n",
    "    df.fillna({\n",
    "        'SentimentMapped': 3, 'SentimentLevel': 3, 'HelpfulnessRatio': 0.0, 'TextLength': 0,\n",
    "        'ExclamationCount': 0, 'QuestionCount': 0, 'CapitalRatio': 0.0, 'UppercaseWords': 0,\n",
    "        'Sentiment_Helpfulness': 0.0, 'Sentiment_TextLength': 0.0, 'ProductAvgScore': train_data['Score'].mean(), 'UserAvgScore': train_data['Score'].mean(),\n",
    "        'Sentiment_Neg': 0.0, 'Sentiment_Neu': 0.0, 'Sentiment_Pos': 0.0, 'Sentiment_Compound': 0.0\n",
    "    }, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Extract features from training data\n",
    "print(\"Extracting features from training data...\")\n",
    "train_data = extract_features(train_data)\n",
    "print(\"Feature extraction on training data complete.\")\n",
    "\n",
    "# Prepare training data by dropping rows with missing 'Score'\n",
    "print(\"Preparing training data...\")\n",
    "train_data = train_data.dropna(subset=['Score'])\n",
    "train_data['Score'] = train_data['Score'].astype(int)\n",
    "\n",
    "# Ensure 'Text' is string type\n",
    "train_data['Text'] = train_data['Text'].fillna('').astype(str)\n",
    "\n",
    "# Compute product and user average scores on training data\n",
    "print(\"Computing overall product and user average scores from training data...\")\n",
    "if 'ProductId' in train_data.columns:\n",
    "    product_avg_score = train_data.groupby('ProductId')['Score'].mean().reset_index().rename(columns={'Score': 'ProductAvgScore'})\n",
    "else:\n",
    "    product_avg_score = None\n",
    "\n",
    "if 'UserId' in train_data.columns:\n",
    "    user_avg_score = train_data.groupby('UserId')['Score'].mean().reset_index().rename(columns={'Score': 'UserAvgScore'})\n",
    "else:\n",
    "    user_avg_score = None\n",
    "\n",
    "# Set features and target\n",
    "feature_columns = ['SentimentMapped', 'SentimentLevel', 'HelpfulnessRatio', 'TextLength',\n",
    "                   'ExclamationCount', 'QuestionCount', 'CapitalRatio', 'UppercaseWords',\n",
    "                   'Sentiment_Helpfulness', 'Sentiment_TextLength', 'ProductAvgScore', 'UserAvgScore',\n",
    "                   'Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos', 'Sentiment_Compound']\n",
    "X_numeric = train_data[feature_columns]\n",
    "y = train_data['Score']\n",
    "\n",
    "# Compute TF-IDF features\n",
    "print(\"Computing TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_text_tfidf = tfidf_vectorizer.fit_transform(train_data['Text'])\n",
    "print(\"TF-IDF feature computation complete.\")\n",
    "\n",
    "# Combine numeric and text features\n",
    "print(\"Combining numeric and text features...\")\n",
    "X_numeric_scaled = StandardScaler().fit_transform(X_numeric)\n",
    "X_combined = hstack([csr_matrix(X_numeric_scaled), X_text_tfidf])\n",
    "\n",
    "# Train the logistic regression model with best parameters\n",
    "print(\"Training logistic regression model with best hyperparameters...\")\n",
    "log_reg_model = LogisticRegression(\n",
    "    max_iter=3000,\n",
    "    solver='newton-cg',\n",
    "    C=0.5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "log_reg_model.fit(X_combined, y)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate model with cross-validation\n",
    "print(\"Evaluating model with cross-validation...\")\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(log_reg_model, X_combined, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Prepare test data\n",
    "print(\"Merging test data with full train data to get 'Text' column...\")\n",
    "# Ensure 'Id's in train_data_full are unique\n",
    "train_data_full_unique = train_data_full.drop_duplicates(subset='Id')\n",
    "test_data = test_data.merge(train_data_full_unique[['Id', 'Text', 'ProductId', 'UserId', 'HelpfulnessNumerator', 'HelpfulnessDenominator']], on='Id', how='left')\n",
    "\n",
    "# Check if any 'Text' values are missing after the merge\n",
    "missing_text = test_data['Text'].isnull().sum()\n",
    "if missing_text > 0:\n",
    "    print(f\"Warning: {missing_text} entries in test data do not have corresponding 'Text' in train data.\")\n",
    "    # Optionally, fill 'Text' with empty strings\n",
    "    test_data['Text'] = test_data['Text'].fillna('')\n",
    "\n",
    "# Apply feature extraction on test data and handle missing values\n",
    "print(\"Extracting features from test data...\")\n",
    "test_data = extract_features(test_data, is_test=True, product_avg_score=product_avg_score, user_avg_score=user_avg_score)\n",
    "print(\"Feature extraction for test data complete.\")\n",
    "\n",
    "# Ensure 'Text' is string type\n",
    "test_data['Text'] = test_data['Text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare test features\n",
    "print(\"Preparing test features...\")\n",
    "X_test_numeric = test_data[feature_columns]\n",
    "X_test_numeric_scaled = StandardScaler().fit_transform(X_test_numeric)\n",
    "\n",
    "X_test_text_tfidf = tfidf_vectorizer.transform(test_data['Text'])\n",
    "print(\"Combining test numeric and text features...\")\n",
    "X_test_combined = hstack([csr_matrix(X_test_numeric_scaled), X_test_text_tfidf])\n",
    "\n",
    "# Predict scores on test data\n",
    "print(\"Predicting scores on test data...\")\n",
    "test_data['Score'] = log_reg_model.predict(X_test_combined)\n",
    "\n",
    "# Clip predictions to be within the valid range and save output to CSV\n",
    "test_data['Score'] = test_data['Score'].round().clip(1, 5).astype(int)\n",
    "print(\"Saving predictions to CSV...\")\n",
    "test_data[['Id', 'Score']].to_csv('option2.csv', index=False)\n",
    "\n",
    "print(\"Score prediction complete; file saved as 'option2.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7560b-0bc3-4d29-8b1a-c2cc6ba784b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
